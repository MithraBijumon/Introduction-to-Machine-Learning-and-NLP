{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c20c5662-9e74-4f9e-817e-e005fc395cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84547a756ea489393f7cccec7cd4a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/21 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd9870328394c569548825145e5ab09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset00.tar:   0%|          | 0.00/633M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mithr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mithr\\.cache\\huggingface\\hub\\datasets--openwebtext. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808deeceb0d44058a0b55b82d0c03cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset01.tar:   0%|          | 0.00/629M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79fb4b7747440c0aa9c0b4260f98d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset02.tar:   0%|          | 0.00/629M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c943fe7c974dd3b24dad12e6482a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset03.tar:   0%|          | 0.00/628M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1744902109483b8b0a744e14ca2722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset04.tar:   0%|          | 0.00/627M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228e0c82219d48e59ab44e5a4781359e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset05.tar:   0%|          | 0.00/630M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48955f91e40049bda50824e42d5ffa8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset06.tar:   0%|          | 0.00/626M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01643c940ef488b8ff7d8e1d03c9f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset07.tar:   0%|          | 0.00/625M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a760c99e7f4e9ebe427dd96fe2583c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset08.tar:   0%|          | 0.00/625M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f66aaad4fc14178bb992a3ad75de05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset09.tar:   0%|          | 0.00/626M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ec7c7c5f064e6997d67c3e2f504ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset10.tar:   0%|          | 0.00/625M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1e15d20d6340768fc1044d3217957f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset11.tar:   0%|          | 0.00/625M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8004e45ad2f449c1ae6f596edff7732e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset12.tar:   0%|          | 0.00/624M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98963ead973b487fbf8086c858295170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset13.tar:   0%|          | 0.00/629M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095712813ac5422f9db9c611965b5586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset14.tar:   0%|          | 0.00/627M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36d855bd7fa4691bd6817a1d4c8d83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset15.tar:   0%|          | 0.00/621M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7770d1424e4f62b640679db0a99061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset16.tar:   0%|          | 0.00/619M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ae608e46794f21b34068f3f492e2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset17.tar:   0%|          | 0.00/619M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9066e82d2b8a4cfaba8200b9ea1f2ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset18.tar:   0%|          | 0.00/618M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9283efa7e577425f9ec852a7abde2284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset19.tar:   0%|          | 0.00/619M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b174a330db41dcba2430101f8350e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset20.tar:   0%|          | 0.00/377M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e811ce4adef43a0bb865a057be334c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8013769 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659d278d9d134ea985fee5f22d33d4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Combine train splits\u001b[39;00m\n\u001b[32m     12\u001b[39m combined_train = concatenate_datasets([wiki[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m], openweb[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m]])\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m combined_valid = concatenate_datasets([wiki[\u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m], \u001b[43mopenweb\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalidation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m])\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Shuffle + select a subset\u001b[39;00m\n\u001b[32m     16\u001b[39m combined_train = combined_train.shuffle(seed=\u001b[32m42\u001b[39m).select(\u001b[38;5;28mrange\u001b[39m(\u001b[32m100_000\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\dataset_dict.py:81\u001b[39m, in \u001b[36mDatasetDict.__getitem__\u001b[39m\u001b[34m(self, k)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) -> Dataset:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     83\u001b[39m         available_suggested_splits = [\n\u001b[32m     84\u001b[39m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split.TRAIN, Split.TEST, Split.VALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m     85\u001b[39m         ]\n",
      "\u001b[31mKeyError\u001b[39m: 'validation'"
     ]
    }
   ],
   "source": [
    "# 1. Data Loading\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load WikiText-2\n",
    "wiki = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Load OpenWebText\n",
    "openweb = load_dataset(\"openwebtext\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2485747f-ceae-4e76-b631-9c052ba22fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Manually split OpenWebText into train + validation\n",
    "openweb_split = openweb[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Combine train splits\n",
    "combined_train = concatenate_datasets([wiki[\"train\"], openweb_split[\"train\"]])\n",
    "combined_valid = concatenate_datasets([wiki[\"validation\"], openweb_split[\"test\"]])\n",
    "\n",
    "# Shuffle + select a subset\n",
    "combined_train = combined_train.shuffle(seed=42).select(range(100_000))\n",
    "combined_valid = combined_valid.shuffle(seed=42).select(range(10_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "842299db-8894-492c-8044-0e0bd5f7b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": combined_train,       # from earlier steps\n",
    "    \"validation\": combined_valid,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "628f77ab-4569-482e-a4cc-83cddf8667f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e78fdd0e2664d578afd0e4320014f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d04cd82fac94faf8ac882675e65ab74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Tokenization\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize(text):\n",
    "    return tokenizer(\n",
    "        text[\"text\"], \n",
    "        return_special_tokens_mask=True,\n",
    "        truncation=True,           \n",
    "        max_length=1024)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0beedc6-2950-4eba-80b5-a811beaf8133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4f7d2d258c4c1e8dab1a4db7484943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8de6ed2bc7460a9e7d3e0839674c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = (len(concatenated[\"input_ids\"]) // block_size) * block_size\n",
    "    \n",
    "    result = {}\n",
    "    for k in concatenated.keys():\n",
    "        chunks = [concatenated[k][i:i+block_size] for i in range(0, total_length, block_size)]\n",
    "        result[k] = chunks\n",
    "    \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_dataset.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71d9ab8c-a7a4-43f1-9222-8caea746fde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Model Selection\n",
    "\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8d18bd8-4f84-46fb-8488-e337c1c44fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch device: CPU\n",
      "transformers device map: n/a\n"
     ]
    }
   ],
   "source": [
    "import torch, transformers\n",
    "print(\"PyTorch device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"transformers device map:\", model.hf_device_map if hasattr(model, 'hf_device_map') else \"n/a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "311be004-5961-45d5-98b6-7e87bc1980d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 7:23:11, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.487800</td>\n",
       "      <td>3.368439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=3.4882898763020833, metrics={'train_runtime': 26605.6773, 'train_samples_per_second': 0.451, 'train_steps_per_second': 0.226, 'total_flos': 783876096000000.0, 'train_loss': 3.4882898763020833, 'epoch': 0.022189102931550315})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Fine-Tuning\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./next-word-model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=3e-5,\n",
    "    fp16=False,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.args.max_steps = 6000\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86bb31f3-3b69-4bf3-b9f9-3782bb21576e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27203' max='27203' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27203/27203 3:05:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 29.03\n"
     ]
    }
   ],
   "source": [
    "# 4. Evaluation\n",
    "\n",
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "332b97f0-c733-4a2a-93f3-ef5783320c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional Extension: Gradio\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "\n",
    "def predict_next_word(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "    predicted_id = torch.argmax(logits, dim=-1).item()\n",
    "    return tokenizer.decode([predicted_id])\n",
    "\n",
    "gr.Interface(fn=predict_next_word, inputs=\"text\", outputs=\"text\", title=\"Next Word Predictor\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd81745-23dd-4117-ab3a-9b6aae90219d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9988e3-c193-4879-b108-3be7f92a2b52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
